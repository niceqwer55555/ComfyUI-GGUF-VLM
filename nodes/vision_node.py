"""
Vision Language Node - è§†è§‰è¯­è¨€æ¨¡å‹èŠ‚ç‚¹ï¼ˆä¿®å¤ç‰ˆï¼‰
"""

import os, platform, gc
import sys
import uuid
import numpy as np
from pathlib import Path
from PIL import Image
import torch
import folder_paths
from comfy.comfy_types import IO

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
module_path = Path(__file__).parent.parent
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))

try:
    from core.model_loader import ModelLoader
    from core.inference_engine import InferenceEngine
    from core.cache_manager import CacheManager
    from utils.registry import RegistryManager
    from utils.downloader import FileDownloader
    from models.vision_models import VisionModelConfig, VisionModelPresets
    from utils.device_optimizer import DeviceOptimizer
    from utils.mmproj_validator import MMProjValidator
    from utils.memory_manager import memory_manager  # æ–°å¢å†…å­˜ç®¡ç†å™¨
except ImportError as e:
    print(f"[ComfyUI-GGUF-VLM] Import error in vision_node: {e}")
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from ..core.model_loader import ModelLoader
    from ..core.inference_engine import InferenceEngine
    from ..core.cache_manager import CacheManager
    from ..utils.registry import RegistryManager
    from ..utils.downloader import FileDownloader
    from ..models.vision_models import VisionModelConfig, VisionModelPresets
    from ..utils.device_optimizer import DeviceOptimizer
    from ..utils.mmproj_finder import MMProjFinder
    from ..utils.memory_manager import memory_manager  # æ–°å¢å†…å­˜ç®¡ç†å™¨

class VisionModelLoader:
    """è§†è§‰è¯­è¨€æ¨¡å‹åŠ è½½å™¨èŠ‚ç‚¹"""

    # å…¨å±€å®ä¾‹
    _model_loader = None
    _cache_manager = None
    _registry = None
    _device_optimizer = None
    _loaded_configs = {}

    @classmethod
    def _get_instances(cls):
        """è·å–å…¨å±€å®ä¾‹"""
        if cls._model_loader is None:
            cls._model_loader = ModelLoader()
        if cls._cache_manager is None:
            cls._cache_manager = CacheManager()
        if cls._registry is None:
            cls._registry = RegistryManager()
        if cls._device_optimizer is None:
            cls._device_optimizer = DeviceOptimizer()
        return cls._model_loader, cls._cache_manager, cls._registry, cls._device_optimizer

    @classmethod
    def INPUT_TYPES(cls):
        loader, cache, registry, optimizer = cls._get_instances()

        # è·å–æœ¬åœ°æ¨¡å‹
        all_local_models = loader.list_models()

        # è¿‡æ»¤æœ¬åœ°æ¨¡å‹ï¼šåªæ˜¾ç¤ºè§†è§‰è¯­è¨€ç±»å‹çš„æ¨¡å‹
        local_models = []
        for model_file in all_local_models:
            model_info = registry.find_model_by_filename(model_file)
            if model_info is None or model_info.get('business_type') in ['image_analysis', 'video_analysis']:
                local_models.append(model_file)

        # è·å–ä¸åŒç±»å‹çš„å¯ä¸‹è½½æ¨¡å‹
        image_models = registry.get_downloadable_models(business_type='image_analysis', model_loader=loader)
        video_models = registry.get_downloadable_models(business_type='video_analysis', model_loader=loader)

        # æ·»åŠ ç±»å‹æ ‡ç­¾
        categorized_models = []

        if image_models:
            categorized_models.append("--- ğŸ–¼ï¸ å›¾åƒåˆ†ææ¨¡å‹ ---")
            categorized_models.extend([name for name, _ in image_models])

        if video_models:
            categorized_models.append("--- ğŸ¥ è§†é¢‘åˆ†ææ¨¡å‹ ---")
            categorized_models.extend([name for name, _ in video_models])

        if local_models:
            categorized_models.append("--- ğŸ’¾ æœ¬åœ°æ¨¡å‹ ---")
            categorized_models.extend(local_models)

        if not categorized_models:
            categorized_models = ["No models found"]

        return {
            "required": {
                "model": (categorized_models, {
                    "default": categorized_models[0] if categorized_models else "No models found",
                    "tooltip": "é€‰æ‹©è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆæŒ‰ç±»å‹åˆ†ç»„ï¼‰"
                }),
                "n_ctx": ("INT", {
                    "default": 8192,
                    "min": 512,
                    "max": 32768,
                    "step": 512,
                    "tooltip": "ä¸Šä¸‹æ–‡çª—å£å¤§å°"
                }),
                "device": (["Auto", "GPU", "CPU"], {
                    "default": "Auto",
                    "tooltip": "è¿è¡Œè®¾å¤‡ (Auto=è‡ªåŠ¨æ£€æµ‹, GPU=å…¨éƒ¨GPU, CPU=ä»…CPU)"
                }),
                "n_gpu_layers": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 200,
                    "step": 1,
                    "tooltip": "GPUå±‚æ•°é‡ï¼ˆ-1=å…¨éƒ¨åŠ è½½ï¼Œ0=ä»…CPUï¼Œæ­£æ•°=æŒ‡å®šå±‚æ•°ï¼‰"
                }),
            },
            "optional": {
                "mmproj_file": ("STRING", {
                    "default": "",
                    "tooltip": "æ‰‹åŠ¨æŒ‡å®š mmproj æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰"
                }),
                "auto_cleanup": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "åˆ‡æ¢æ¨¡å‹æ—¶è‡ªåŠ¨æ¸…ç†æ—§æ¨¡å‹æ˜¾å­˜"
                }),
                "aggressive_cleanup": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ä½¿ç”¨æ¿€è¿›çš„æ˜¾å­˜æ¸…ç†ç­–ç•¥ï¼ˆæ¨èå¼€å¯ï¼‰"
                }),
            }
        }

    RETURN_TYPES = ("VISION_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "ğŸ¤– GGUF-VLM/ğŸ–¼ï¸ Vision Models"

    def load_model(self, model, n_ctx=8192, device="Auto", n_gpu_layers=-1, 
                  mmproj_file="", auto_cleanup=True, aggressive_cleanup=True):
        """åŠ è½½è§†è§‰è¯­è¨€æ¨¡å‹"""
        loader, cache, registry, optimizer = self._get_instances()

        # æ£€æŸ¥CUDAæ”¯æŒçŠ¶æ€
        cuda_info = memory_manager.check_cuda_support()
        print(f"\n=== CUDAæ”¯æŒçŠ¶æ€ ===")
        print(f"PyTorch CUDA: {cuda_info['pytorch_cuda']}")
        print(f"GPUæ•°é‡: {cuda_info['gpu_count']}")
        if cuda_info['pytorch_cuda']:
            for i, name in enumerate(cuda_info['gpu_names']):
                print(f"GPU {i}: {name}")
        print(f"llama-cpp-python CUDA: {cuda_info['llama_cuda']}")
        print(f"llama-cpp-pythonç‰ˆæœ¬: {cuda_info['llama_version']}")

        # æ£€æŸ¥ llama-cpp-python å®‰è£…çŠ¶æ€
        llama_status = optimizer.check_llama_cpp_installation()
        if not llama_status['installed']:
            error_msg = "âŒ llama-cpp-python not installed.\n"
            error_msg += "Install with:\n"
            error_msg += "  pip install llama-cpp-python\n"
            error_msg += "\nFor CUDA support, use:\n"
            error_msg += "  pip install llama-cpp-python --force-reinstall --index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu121"
            raise RuntimeError(error_msg)

        if llama_status['issues']:
            for issue in llama_status['issues']:
                print(f"âš ï¸  {issue}")

        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        device_summary = optimizer.get_device_summary()
        print(f"\n{device_summary}\n")

        # å¤„ç†n_gpu_layerså‚æ•° - æ ¹æ®CUDAæ”¯æŒè°ƒæ•´
        if not cuda_info['llama_cuda']:
            print("âš ï¸  llama-cpp-pythonæ²¡æœ‰CUDAæ”¯æŒï¼Œå¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
            n_gpu_layers = 0
            device = "CPU"
        elif n_gpu_layers == -1:
            if device == "Auto":
                optimized_params = optimizer.get_optimized_params(model_size_gb=7.0)
                n_gpu_layers = optimized_params['n_gpu_layers']
                n_batch = optimized_params.get('n_batch', 512)
                print(f"ğŸ¯ Auto-optimized: {optimized_params['device_info']}")
                print(f"   GPU layers: {n_gpu_layers}")
                print(f"   Batch size: {n_batch}")
            elif device == "GPU":
                n_gpu_layers = -1
                n_batch = 512
                print(f"ğŸ® Using GPU (all layers)")
            else:
                n_gpu_layers = 0
                n_batch = 128
                print(f"ğŸ’» Using CPU only")
        else:
            n_batch = 512 if n_gpu_layers > 0 else 128
            print(f"âš™ï¸ Manual GPU layers: {n_gpu_layers}")
            print(f"   Batch size: {n_batch}")

        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†ç»„æ ‡é¢˜
        if model.startswith("---"):
            raise ValueError("è¯·é€‰æ‹©ä¸€ä¸ªå…·ä½“çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯åˆ†ç»„æ ‡é¢˜")

        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model}")

        # ç›‘æ§å†…å­˜ä½¿ç”¨
        memory_manager.monitor_memory_usage("before model loading")

        # è‡ªåŠ¨æ¸…ç†æ—§æ¨¡å‹ï¼ˆå¦‚æœå¼€å¯ï¼‰
        if auto_cleanup and VisionLanguageNode._inference_engine is not None:
            print("ğŸ§¹ Auto-cleanup: cleaning up previous models...")
            VisionLanguageNode.cleanup_all_models(aggressive=aggressive_cleanup)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
        if model.startswith("âœ—"):
            print(f"ğŸ“¥ Model needs to be downloaded: {model}")
            download_info = registry.get_model_download_info(model)

            if download_info:
                downloader = FileDownloader()
                model_dir = loader.model_dirs[0]

                # ä¸‹è½½æ¨¡å‹æ–‡ä»¶
                downloaded_path = downloader.download_from_huggingface(
                    repo_id=download_info['repo'],
                    filename=download_info['filename'],
                    dest_dir=model_dir
                )

                if not downloaded_path:
                    raise RuntimeError(f"Failed to download model: {model}")

                # ä¸‹è½½ mmproj æ–‡ä»¶
                if download_info.get('mmproj'):
                    mmproj_repo = download_info.get('mmproj_repo', download_info['repo'])
                    mmproj_downloaded = downloader.download_from_huggingface(
                        repo_id=mmproj_repo,
                        filename=download_info['mmproj'],
                        dest_dir=model_dir
                    )
                    if mmproj_downloaded:
                        mmproj_file = download_info['mmproj']
                        print(f"âœ… Downloaded mmproj from {mmproj_repo}")

                model = download_info['filename']
                cache.clear("new model downloaded")
            else:
                raise ValueError(f"Cannot find download info for: {model}")
        elif model.startswith("âœ“"):
            import re
            model = re.sub(r'^âœ“\s*', '', model)

        # æŸ¥æ‰¾æ¨¡å‹è·¯å¾„
        model_path = loader.find_model(model)
        if not model_path:
            raise FileNotFoundError(f"Model not found: {model}")

        # æŸ¥æ‰¾ mmproj æ–‡ä»¶
        mmproj_path = None
        if mmproj_file:
            mmproj_path = loader.find_mmproj(model, mmproj_file)
            if not mmproj_path:
                raise FileNotFoundError(f"mmproj file not found: {mmproj_file}")
        else:
            print(f"ğŸ” Auto-searching for mmproj file...")
            mmproj_path = loader.find_mmproj(model)

            if not mmproj_path:
                mmproj_name = registry.smart_match_mmproj(model)
                if mmproj_name:
                    mmproj_path = loader.find_mmproj(model, mmproj_name)

                if not mmproj_path:
                    print(f"âš ï¸  mmproj not found locally, attempting auto-download...")
                    model_info = registry.find_model_by_filename(model)
                    if model_info and model_info.get('mmproj'):
                        downloader = FileDownloader()
                        model_dir = os.path.dirname(model_path)
                        mmproj_path = downloader.download_from_huggingface(
                            repo_id=model_info['repo'],
                            filename=model_info['mmproj'],
                            dest_dir=model_dir
                        )

        if not mmproj_path:
            from ..utils.mmproj_finder import MMProjFinder
            from ..utils.mmproj_validator import MMProjValidator

            finder = MMProjFinder([os.path.dirname(model_path)])
            validator = MMProjValidator()

            suggestions = validator.suggest_mmproj_for_model(model)
            available = finder.list_all_mmproj_files(os.path.dirname(model_path))

            error_msg = f"âŒ Could not find mmproj file for {model}.\n\n"
            error_msg += f"ğŸ’¡ Recommended mmproj filename:\n"
            error_msg += f"   {suggestions['primary']}\n\n"

            if available:
                error_msg += f"ğŸ“ Available mmproj files in model directory:\n"
                for mmproj_path_item in available:
                    mmproj_name = os.path.basename(mmproj_path_item)
                    compat = validator.check_compatibility(model, mmproj_name)

                    if compat['confidence'] == 'high':
                        error_msg += f"   âœ… {mmproj_name} (æ¨èä½¿ç”¨)\n"
                    elif compat['confidence'] == 'medium':
                        error_msg += f"   âš ï¸  {mmproj_name} (å¯èƒ½å…¼å®¹)\n"
                    else:
                        error_msg += f"   âŒ {mmproj_name} (å¯èƒ½ä¸å…¼å®¹)\n"

                error_msg += "\n"

            error_msg += "è§£å†³æ–¹æ¡ˆ:\n"
            error_msg += "1. ä¸‹è½½ä¸æ¨¡å‹åŒ¹é…çš„ mmproj æ–‡ä»¶\n"
            error_msg += "2. å¦‚æœæœ‰æ¨èçš„æ–‡ä»¶ï¼Œé‡å‘½åä¸ºæ¨èçš„æ–‡ä»¶å\n"
            error_msg += "3. åœ¨èŠ‚ç‚¹ä¸­æ‰‹åŠ¨æŒ‡å®š mmproj_file å‚æ•°\n"

            raise FileNotFoundError(error_msg)

        # åº”ç”¨é¢„è®¾é…ç½®
        preset = VisionModelPresets.get_preset(model)
        if preset:
            print(f"ğŸ“‹ Applying preset for {model}")
            if n_ctx == 8192:
                n_ctx = preset.get('n_ctx', n_ctx)

        # åˆ›å»ºé…ç½®
        config = VisionModelConfig(
            model_name=model,
            model_path=model_path,
            mmproj_path=mmproj_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            auto_cleanup=auto_cleanup
        )

        # éªŒè¯é…ç½®
        validation = config.validate()
        if not validation['valid']:
            raise ValueError(f"Invalid config: {validation['errors']}")

        # è®°å½•å·²åŠ è½½çš„é…ç½®
        self._loaded_configs[model_path] = config.to_dict()

        print(f"âœ… Vision model loaded: {model}")
        print(f"ğŸ“ Using mmproj: {os.path.basename(mmproj_path)}")
        print(f"âš™ï¸ GPU layers: {n_gpu_layers}, Auto-cleanup: {auto_cleanup}")

        # ç›‘æ§å†…å­˜ä½¿ç”¨
        memory_manager.monitor_memory_usage("after model loading")

        return (config.to_dict(),)

    @classmethod
    def cleanup_loaded_configs(cls):
        """æ¸…ç†å·²åŠ è½½çš„é…ç½®ç¼“å­˜"""
        cls._loaded_configs.clear()
        print(f"ğŸ§¹ Cleared all loaded model configs")

class VisionLanguageNode:
    """è§†è§‰è¯­è¨€ç”ŸæˆèŠ‚ç‚¹ï¼ˆå¢å¼ºæ˜¾å­˜ç®¡ç†ç‰ˆï¼‰"""

    # å…¨å±€æ¨ç†å¼•æ“
    _inference_engine = None

    @classmethod
    def _get_engine(cls):
        """è·å–æ¨ç†å¼•æ“"""
        if cls._inference_engine is None:
            cls._inference_engine = InferenceEngine()
        return cls._inference_engine

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("VISION_MODEL", {
                    "tooltip": "è§†è§‰è¯­è¨€æ¨¡å‹é…ç½®"
                }),
                "prompt": (IO.STRING, {
                    "default": "Describe this image in detail.",
                    "multiline": False,
                    "tooltip": "ç”¨æˆ·æç¤ºè¯"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 1,
                    "max": 4096,
                    "step": 1,
                    "tooltip": "æœ€å¤§ç”Ÿæˆ token æ•°"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "æ¸©åº¦å‚æ•°"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "Top-p é‡‡æ ·"
                }),
                "top_k": ("INT", {
                    "default": 40,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "Top-k é‡‡æ ·"
                }),
                "seed": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xFFFFFFFFFFFFFFFF,
                    "tooltip": "éšæœºç§å­"
                }),
            },
            "optional": {
                "image": ("IMAGE", {
                    "tooltip": "è¾“å…¥å›¾åƒï¼ˆä¸è§†é¢‘äºŒé€‰ä¸€ï¼‰"
                }),
                "video": ("IMAGE", {
                    "tooltip": "è¾“å…¥è§†é¢‘å¸§åºåˆ—ï¼ˆä¸å›¾åƒäºŒé€‰ä¸€ï¼‰"
                }),
                "system_prompt": (IO.STRING, {
                    "default": "You are a helpful assistant that describes images and videos accurately and in detail.",
                    "multiline": True,
                    "tooltip": "ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯è‡ªå®šä¹‰æ¨¡å‹è¡Œä¸ºï¼‰"
                }),
                "cleanup_after_inference": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ¨ç†å®Œæˆåè‡ªåŠ¨æ¸…ç†æ¨¡å‹æ˜¾å­˜ï¼ˆæ¨èå¼€å¯ï¼‰"
                }),
                "aggressive_cleanup": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ä½¿ç”¨æ¿€è¿›çš„æ˜¾å­˜æ¸…ç†ç­–ç•¥ï¼ˆæ¨èå¼€å¯ï¼‰"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("context",)
    FUNCTION = "describe_image"
    CATEGORY = "ğŸ¤– GGUF-VLM/ğŸ–¼ï¸ Vision Models"
    OUTPUT_NODE = True

    def describe_image(self, model, prompt, max_tokens=512,
                      temperature=0.7, top_p=0.9, top_k=40, seed=0,
                      image=None, video=None, system_prompt=None, 
                      cleanup_after_inference=True, aggressive_cleanup=True):
        """ç”Ÿæˆå›¾åƒ/è§†é¢‘æè¿°"""
        llm = None
        model_path = model.get('model_path')
        
        # ç›‘æ§å†…å­˜ä½¿ç”¨
        memory_info = memory_manager.monitor_memory_usage("start of inference")
        
        # æ£€æµ‹è¿è¡Œæ¨¡å¼
        if not memory_info["cuda_available"]:
            print("âš ï¸  WARNING: Running in CPU mode - performance will be slower")
            print("ğŸ’¡ Tip: Install CUDA-enabled llama-cpp-python for GPU acceleration")
        
        try:
            from llama_cpp import Llama
            from llama_cpp.llama_chat_format import Qwen25VLChatHandler

            # éªŒè¯è¾“å…¥
            if image is None and video is None:
                raise ValueError("å¿…é¡»æä¾› image æˆ– video è¾“å…¥ä¹‹ä¸€")
            if image is not None and video is not None:
                raise ValueError("ä¸èƒ½åŒæ—¶æä¾› image å’Œ video è¾“å…¥ï¼Œè¯·åªé€‰æ‹©ä¸€ä¸ª")

            engine = self._get_engine()

            # ç¡®å®šè¾“å…¥ç±»å‹
            is_video = video is not None
            input_data = video if is_video else image

            print(f"ğŸ“Š è¾“å…¥ç±»å‹: {'è§†é¢‘' if is_video else 'å›¾åƒ'}")
            if is_video:
                print(f"ğŸ¬ è§†é¢‘å¸§æ•°: {input_data.shape[0]}")

            # è·å– auto_cleanup è®¾ç½®
            auto_cleanup = model.get('auto_cleanup', True)

            # è‡ªåŠ¨æ¸…ç†æ—§æ¨¡å‹
            if auto_cleanup and engine.is_model_loaded(model_path):
                print(f"ğŸ§¹ Auto-cleanup: unloading previous model")
                self.cleanup_model(model_path, aggressive=aggressive_cleanup)

            # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
            if not engine.is_model_loaded(model_path):
                print(f"ğŸ”„ Loading vision model into memory...")
                print(f"ğŸ“ Model: {os.path.basename(model_path)}")
                print(f"ğŸ“ mmproj: {os.path.basename(model['mmproj_path'])}")
                
                # æ˜¾ç¤ºè¿è¡Œæ¨¡å¼
                cuda_info = memory_manager.check_cuda_support()
                if cuda_info['llama_cuda'] and model.get('n_gpu_layers', -1) > 0:
                    print(f"ğŸ® Running on GPU with {model.get('n_gpu_layers', -1)} layers")
                else:
                    print(f"ğŸ’» Running on CPU")

                chat_handler = Qwen25VLChatHandler(clip_model_path=model['mmproj_path'])
                llm = Llama(
                    model_path=model_path,
                    chat_handler=chat_handler,
                    n_ctx=model.get('n_ctx', 8192),
                    n_gpu_layers=model.get('n_gpu_layers', -1),
                    verbose=model.get('verbose', False),
                    seed=seed
                )

                engine.loaded_models[model_path] = llm
                print(f"âœ… Vision model loaded successfully")
                
                # ç›‘æ§å†…å­˜ä½¿ç”¨
                memory_manager.monitor_memory_usage("after model loading")
            else:
                llm = engine.loaded_models[model_path]

            # å¤„ç†å›¾åƒæˆ–è§†é¢‘å¸§
            if is_video:
                image_paths = self._save_video_frames(input_data, seed)
            else:
                image_paths = [self._save_temp_image(input_data, seed)]

            # æ„å»ºæ¶ˆæ¯å†…å®¹
            content = []

            # æ·»åŠ å›¾åƒ/è§†é¢‘å¸§
            for img_path in image_paths:
                if not img_path or not os.path.exists(img_path):
                    raise FileNotFoundError(f"æ— æ•ˆçš„å›¾åƒè·¯å¾„ï¼š{img_path}")

                if platform.system() == "Windows":
                    abs_path = os.path.abspath(img_path)
                    img_url = f"file:///{abs_path.replace(os.sep, '/')}"
                else:
                    abs_path = os.path.abspath(img_path)
                    img_url = f"file://{abs_path}"

                content.append({
                    "type": "image_url",
                    "image_url": {"url": img_url}
                })
            
            # æ·»åŠ ç”¨æˆ·æç¤ºè¯
            content.append({
                "type": "text",
                "text": prompt
            })

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []

            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæä¾›ï¼‰
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
                print(f"ğŸ“‹ ç³»ç»Ÿæç¤ºè¯: {system_prompt[:50]}...")

            messages.append({"role": "user", "content": content})

            print(f"ğŸ¤– Generating {'video' if is_video else 'image'} description...")
            print(f"ğŸ“ ç”¨æˆ·æç¤ºè¯: {prompt[:50]}...")

            # ç”Ÿæˆæè¿°
            response = llm.create_chat_completion(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                stream=False
            )

            output_text = response["choices"][0]["message"]["content"]
            output_text = output_text.strip()

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for img_path in image_paths:
                try:
                    os.remove(img_path)
                except:
                    pass

            print(f"âœ… Generated description ({len(output_text)} chars)")

            # æ¨ç†åè‡ªåŠ¨æ¸…ç†æ¨¡å‹
            if cleanup_after_inference:
                self.cleanup_model(model_path, aggressive=aggressive_cleanup)
                print(f"ğŸ§¹ Model cleaned up after inference")

            return (output_text,)

        except ImportError as e:
            error_msg = "âŒ llama-cpp-python not installed. Install with: pip install llama-cpp-python"
            print(error_msg)
            return (error_msg,)

        except Exception as e:
            import traceback
            error_msg = f"âŒ Error: {str(e)}"
            print(f"âŒ Detailed error:\n{traceback.format_exc()}")

            # å¼‚å¸¸æ—¶ä¹Ÿæ¸…ç†æ¨¡å‹
            if cleanup_after_inference and model_path and engine.is_model_loaded(model_path):
                self.cleanup_model(model_path, aggressive=aggressive_cleanup)

            return (error_msg,)

    def _save_temp_image(self, image, seed):
        """ä¿å­˜å›¾åƒåˆ°ä¸´æ—¶æ–‡ä»¶"""
        unique_id = uuid.uuid4().hex
        image_path = Path(folder_paths.temp_directory) / f"temp_image_{seed}_{unique_id}.png"
        image_path.parent.mkdir(parents=True, exist_ok=True)

        # è½¬æ¢ tensor åˆ° PIL Image
        img_array = image.cpu().numpy()
        if img_array.ndim == 4:
            img_array = img_array[0]
        img_array = np.clip(255.0 * img_array, 0, 255).astype(np.uint8)
        img = Image.fromarray(img_array)
        img.save(str(image_path))

        return str(image_path.resolve())

    def _save_video_frames(self, video, seed, max_frames=8):
        """ä¿å­˜è§†é¢‘å¸§åˆ°ä¸´æ—¶æ–‡ä»¶"""
        unique_id = uuid.uuid4().hex
        temp_dir = Path(folder_paths.temp_directory) / f"temp_video_{seed}_{unique_id}"
        temp_dir.mkdir(parents=True, exist_ok=True)

        video_array = video.cpu().numpy()
        num_frames = video_array.shape[0]

        print(f"ğŸ¬ å¤„ç†è§†é¢‘: {num_frames} å¸§")

        if num_frames > max_frames:
            indices = np.linspace(0, num_frames - 1, max_frames, dtype=int)
            video_array = video_array[indices]
            print(f"ğŸ“Š é‡‡æ ·åˆ° {max_frames} å¸§")

        image_paths = []
        for i, frame in enumerate(video_array):
            img_array = np.clip(255.0 * frame, 0, 255).astype(np.uint8)
            img = Image.fromarray(img_array)

            frame_path = temp_dir / f"frame_{i:04d}.png"
            img.save(str(frame_path))
            image_paths.append(str(frame_path.resolve()))

        print(f"âœ… ä¿å­˜äº† {len(image_paths)} ä¸ªè§†é¢‘å¸§")
        return image_paths

    @classmethod
    def cleanup_model(cls, model_path, aggressive=True):
        """æ¸…ç†æŒ‡å®šæ¨¡å‹çš„æ˜¾å­˜å ç”¨"""
        if cls._inference_engine is None:
            return

        if model_path in cls._inference_engine.loaded_models:
            print(f"ğŸ§¹ Cleaning up model: {os.path.basename(model_path)}")

            # è·å–æ¨¡å‹å®ä¾‹
            llm = cls._inference_engine.loaded_models.pop(model_path)
            
            # ä½¿ç”¨å†…å­˜ç®¡ç†å™¨å¼ºåˆ¶æ¸…ç†
            memory_manager.force_llama_cleanup(llm)

            # æ¿€è¿›çš„æ˜¾å­˜æ¸…ç†
            if aggressive:
                memory_manager.aggressive_memory_cleanup()
            else:
                memory_manager.aggressive_memory_cleanup(max_retries=1)
        else:
            print(f"â„¹ï¸ Model not found in loaded models: {os.path.basename(model_path)}")

    @classmethod
    def cleanup_all_models(cls, aggressive=True):
        """æ¸…ç†æ‰€æœ‰å·²åŠ è½½æ¨¡å‹çš„æ˜¾å­˜å ç”¨"""
        if cls._inference_engine is None:
            return

        if cls._inference_engine.loaded_models:
            print(f"ğŸ§¹ Cleaning up all loaded vision models ({len(cls._inference_engine.loaded_models)} models)")

            # åˆ é™¤æ‰€æœ‰æ¨¡å‹å¼•ç”¨
            for model_path in list(cls._inference_engine.loaded_models.keys()):
                llm = cls._inference_engine.loaded_models.pop(model_path)
                memory_manager.force_llama_cleanup(llm)

            # æ¸…ç†é…ç½®ç¼“å­˜
            VisionModelLoader.cleanup_loaded_configs()

            # æ˜¾å­˜æ¸…ç†
            if aggressive:
                memory_manager.aggressive_memory_cleanup()
            else:
                memory_manager.aggressive_memory_cleanup(max_retries=1)
            
            print(f"âœ… All vision models cleaned up")
        else:
            print(f"â„¹ï¸ No loaded vision models to clean up")

    # æ–°å¢ï¼šæ‰‹åŠ¨è§¦å‘æ˜¾å­˜æ¸…ç†
    @classmethod
    def manual_memory_cleanup(cls):
        """æ‰‹åŠ¨è§¦å‘æ˜¾å­˜æ¸…ç†"""
        print("ğŸ”„ Manual memory cleanup triggered")
        cls.cleanup_all_models(aggressive=True)

    @classmethod
    def __del__(cls):
        try:
            cls.cleanup_all_models(aggressive=True)
        except:
            pass

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "VisionModelLoader": VisionModelLoader,
    "VisionLanguageNode": VisionLanguageNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VisionModelLoader": "ğŸ–¼ï¸ Vision Model Loader (GGUF)",
    "VisionLanguageNode": "ğŸ–¼ï¸ Image Analysis (GGUF)",
}
