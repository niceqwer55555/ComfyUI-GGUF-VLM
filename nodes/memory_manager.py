"""
Memory Manager - æ˜¾å­˜ç®¡ç†å·¥å…·ï¼ˆå¢å¼ºç‰ˆï¼‰
"""

import gc
import torch
import psutil
import os
import subprocess
from typing import Dict, Any

class MemoryManager:
    """æ˜¾å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.initial_memory = self.get_memory_info()
    
    def get_memory_info(self):
        """è·å–å®Œæ•´çš„æ˜¾å­˜å’Œå†…å­˜ä¿¡æ¯"""
        memory_info = {}
        
        # GPUæ˜¾å­˜ä¿¡æ¯
        if torch.cuda.is_available():
            memory_info.update({
                "gpu_allocated": torch.cuda.memory_allocated(),
                "gpu_reserved": torch.cuda.memory_reserved(),
                "gpu_total": torch.cuda.get_device_properties(0).total_memory,
                "cuda_available": True
            })
        else:
            memory_info.update({
                "gpu_allocated": 0,
                "gpu_reserved": 0,
                "gpu_total": 0,
                "cuda_available": False
            })
        
        # ç³»ç»Ÿå†…å­˜ä¿¡æ¯
        system_memory = psutil.virtual_memory()
        memory_info.update({
            "system_total": system_memory.total,
            "system_available": system_memory.available,
            "system_used": system_memory.used,
            "system_percent": system_memory.percent
        })
        
        # è¿›ç¨‹å†…å­˜ä¿¡æ¯
        process = psutil.Process()
        process_memory = process.memory_info()
        memory_info.update({
            "process_rss": process_memory.rss,  # å¸¸é©»å†…å­˜
            "process_vms": process_memory.vms   # è™šæ‹Ÿå†…å­˜
        })
        
        return memory_info
    
    def monitor_memory_usage(self, stage_name):
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_info = self.get_memory_info()
        
        print(f"ğŸ“Š Memory usage at {stage_name}:")
        
        if memory_info["cuda_available"]:
            print(f"   GPU - Allocated: {memory_info['gpu_allocated']/1024**3:.2f}GB, "
                  f"Reserved: {memory_info['gpu_reserved']/1024**3:.2f}GB")
        else:
            print(f"   âš ï¸  CUDA not available - running on CPU")
            
        print(f"   System - Used: {memory_info['system_used']/1024**3:.1f}GB "
              f"({memory_info['system_percent']:.1f}%)")
        print(f"   Process - RSS: {memory_info['process_rss']/1024**3:.2f}GB, "
              f"VMS: {memory_info['process_vms']/1024**3:.2f}GB")
        
        return memory_info
    
    def aggressive_memory_cleanup(self, max_retries=3):
        """æ¿€è¿›çš„æ˜¾å­˜æ¸…ç†"""
        print("ğŸ§¹ Starting aggressive memory cleanup...")
        
        # è®°å½•æ¸…ç†å‰çŠ¶æ€
        before_memory = self.get_memory_info()
        
        for attempt in range(max_retries):
            print(f"  Attempt {attempt + 1}/{max_retries}")
            
            # å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶
            collected = gc.collect()
            print(f"    GC collected {collected} objects")
            
            if torch.cuda.is_available():
                # æ¸…ç†PyTorchç¼“å­˜
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
                # æ¸…ç†CUDA IPCç¼“å­˜
                if hasattr(torch.cuda, 'ipc_collect'):
                    torch.cuda.ipc_collect()
                
                # é‡ç½®å†…å­˜ç»Ÿè®¡
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.reset_accumulated_memory_stats()
        
        # è®°å½•æ¸…ç†åçŠ¶æ€
        after_memory = self.get_memory_info()
        
        # è®¡ç®—é‡Šæ”¾çš„å†…å­˜
        freed_memory = {
            "gpu_allocated": before_memory["gpu_allocated"] - after_memory["gpu_allocated"],
            "gpu_reserved": before_memory["gpu_reserved"] - after_memory["gpu_reserved"],
            "process_rss": before_memory["process_rss"] - after_memory["process_rss"]
        }
        
        print(f"âœ… Memory cleanup completed:")
        
        if after_memory["cuda_available"]:
            print(f"   GPU - Allocated: {after_memory['gpu_allocated']/1024**3:.2f}GB "
                  f"(Freed: {freed_memory['gpu_allocated']/1024**3:.2f}GB)")
            print(f"   GPU - Reserved: {after_memory['gpu_reserved']/1024**3:.2f}GB "
                  f"(Freed: {freed_memory['gpu_reserved']/1024**3:.2f}GB)")
        else:
            print(f"   âš ï¸  Running on CPU - no GPU memory to free")
            
        print(f"   System - Used: {after_memory['system_used']/1024**3:.1f}GB "
              f"({after_memory['system_percent']:.1f}%)")
        print(f"   Process - RSS: {after_memory['process_rss']/1024**3:.2f}GB "
              f"(Freed: {freed_memory['process_rss']/1024**3:.2f}GB)")
        
        return freed_memory
    
    def force_llama_cleanup(self, llama_instance):
        """å¼ºåˆ¶æ¸…ç†Llamaå®ä¾‹"""
        if llama_instance is None:
            return
        
        try:
            print("    Force cleaning Llama instance...")
            
            # å°è¯•å„ç§æ¸…ç†æ–¹æ³•
            cleanup_methods = ['close', '__del__', 'free', 'cleanup']
            
            for method_name in cleanup_methods:
                if hasattr(llama_instance, method_name):
                    try:
                        method = getattr(llama_instance, method_name)
                        method()
                        print(f"      âœ… Called {method_name}()")
                    except Exception as e:
                        print(f"      âš ï¸  {method_name}() failed: {e}")
            
            # å¼ºåˆ¶åˆ é™¤å¼•ç”¨
            del llama_instance
            
            # ç«‹å³åƒåœ¾å›æ”¶
            gc.collect()
            
        except Exception as e:
            print(f"    âš ï¸  Llama cleanup failed: {e}")
    
    def check_cuda_support(self):
        """æ£€æŸ¥CUDAæ”¯æŒçŠ¶æ€"""
        cuda_info = {
            "pytorch_cuda": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "gpu_names": []
        }
        
        if cuda_info["pytorch_cuda"]:
            for i in range(cuda_info["gpu_count"]):
                cuda_info["gpu_names"].append(torch.cuda.get_device_name(i))
        
        # æ£€æŸ¥llama-cpp-python CUDAæ”¯æŒ
        try:
            import llama_cpp
            cuda_info["llama_cuda"] = hasattr(llama_cpp, 'LLAMA_CUBLAS')
            cuda_info["llama_version"] = getattr(llama_cpp, '__version__', 'unknown')
        except ImportError:
            cuda_info["llama_cuda"] = False
            cuda_info["llama_version"] = "not installed"
        
        return cuda_info

# å…¨å±€å†…å­˜ç®¡ç†å™¨å®ä¾‹
memory_manager = MemoryManager()